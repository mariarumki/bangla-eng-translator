{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e40z9f8LGN8F",
        "outputId": "aa0d0982-5046-46bf-88c7-cf55f85f6c7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting praw\n",
            "  Downloading praw-7.6.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.8/188.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from prawcore<3,>=2.1->praw) (2.25.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.6.1 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TenlaldwF7s4"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Application for Reddit"
      ],
      "metadata": {
        "id": "l3FMpym3GYTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_agent = \"Test API 1\"\n",
        "\n",
        "# defining instances\n",
        "reddit = praw.Reddit(\n",
        "    client_id = \"KUivSj_mwwz7tZe9-tR8eA\",\n",
        "    client_secret = \"Bl4xX0C1ad0h2RzngPvzCjgThveCyg\",\n",
        "    user_agent = user_agent,\n",
        "    check_for_async = False\n",
        ")"
      ],
      "metadata": {
        "id": "HOtBAYIhGFDY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Posts from Reddit"
      ],
      "metadata": {
        "id": "lqZr4-xDG3ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_name = \"macbook\"\n",
        "subreddit = reddit.subreddit(subreddit_name)\n",
        "\n",
        "print(subreddit.display_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unSMjSYUGFN2",
        "outputId": "eacb56e2-77b7-49ab-9016-9ec8f78bb76b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macbook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles=[]\n",
        "scores=[]\n",
        "ids=[]\n",
        "\n",
        "for submission in subreddit.top(limit=50):\n",
        "    titles.append(submission.title)\n",
        "    scores.append(submission.score) # upvotes\n",
        "    ids.append(submission.id)"
      ],
      "metadata": {
        "id": "KW7axMTYHXCz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "\n",
        "df['Title'] = titles\n",
        "df['Id'] = ids\n",
        "df['Upvotes'] = scores # upvotes\n",
        "\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_sq62IDHpAJ",
        "outputId": "b708b453-a25d-48d4-84c0-570bf227f3cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title       Id  Upvotes\n",
            "0  Got accepted into medical school and my parent...   pxwnp7     1115\n",
            "1         Goodbye surface ‚Äúpro‚Äù 7, hello bankruptcy.   mx7tuf      864\n",
            "2  Surprised my girlfriend with a MacBook Air for...   aevd2n      743\n",
            "3  Unpopular Opinion / Rant: This subreddit has b...   o0wa4d      629\n",
            "4                          Loving my new retro skins   go8ylr      612\n",
            "5                                   So many macbooks  10zha2d      606\n",
            "6  Upgraded! From 2006 Intel MacBook Pro to 2020 ...   kc079r      568\n",
            "7                      MacBook pro 16 m1pro unboxed.   qe8yxp      548\n",
            "8                                          The M1 rn   jzphvx      514\n",
            "9  ü§¶‚Äç‚ôÇÔ∏è All these M1 posts are having me regret b...   kxlin5      502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "GI0mY4CcIIpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "WNnei11WIMH6",
        "outputId": "bdc8fa67-f04c-48f7-d435-07c0dd6a2de0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchdata) (1.13.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchdata) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1 urllib3-1.26.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IwYQKxtIvUv",
        "outputId": "c11c97c0-7434-4edf-e78b-ceb0a1ba3f48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-25 06:34:06.473300: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-25 06:34:07.907488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-25 06:34:07.907639: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-25 06:34:07.907660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.22.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.1\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchdata, torchtext\n",
        "from torch import nn\n",
        "\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBVuQudfH33X",
        "outputId": "d4a5956e-94f6-47bc-98f7-db1412413128"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f625918d290>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-tAwocwOH3qc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import SST2\n",
        "train = SST2(split='train')"
      ],
      "metadata": {
        "id": "-wylT7S-H3gc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_md')"
      ],
      "metadata": {
        "id": "HlS7fB_lH3WT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):  # data_iter, e.g., train\n",
        "    for text, _  in data_iter:\n",
        "        yield tokenizer(text)\n",
        "        \n",
        "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>',\n",
        "                                                                 '<bos>', '<eos>'])"
      ],
      "metadata": {
        "id": "Atr53xGUIZes"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.set_default_index(vocab[\"<unk>\"]) "
      ],
      "metadata": {
        "id": "4ZhvXTdRIZaH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = vocab.get_itos()"
      ],
      "metadata": {
        "id": "S-X43I17I_jd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_idx = vocab['<pad>']"
      ],
      "metadata": {
        "id": "2WzHSy0VI_rP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)"
      ],
      "metadata": {
        "id": "AfxiRXuGJFXR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "F4HAlrPJJP4g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0176cf94-9828-4383-8e35-e33367441876"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, \n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        # input dim = how many vocab we have\n",
        "        # emb dim = 300 --> we use fasttext\n",
        "        # padding_idx tells this lookup table to ignore, and just randomize....\n",
        "        # <unk>, <bos>, <eos>\n",
        "        self.embedding_layer = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm            = nn.LSTM(emb_dim,\n",
        "                                       hid_dim,\n",
        "                                       num_layers = num_layers,\n",
        "                                       bidirectional = bidirectional,\n",
        "                                       dropout = dropout,  # dropout is applied between layers....\n",
        "                                       batch_first=True)\n",
        "        \n",
        "        self.fc              = nn.Linear(hid_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, x, lengths):\n",
        "        # x: [batch size, seq len]\n",
        "        \n",
        "        embedded_x = self.embedding_layer(x)\n",
        "        # x: [batch size, seq len, emb dim]\n",
        "        \n",
        "        # pack this embedded_x in such a way that RNN knows to ignore padding....\n",
        "        # without batch_first = True; things will become [seq len, batch size, emb dim]\n",
        "        pack_embedded = nn.utils.rnn.pack_padded_sequence(embedded_x, lengths.to('cpu'),\n",
        "                                                          enforce_sorted=False,\n",
        "                                                          batch_first = True\n",
        "                                                          )\n",
        "        \n",
        "        # packed_outputs is basically all hidden states\n",
        "        # h is the last hidden state\n",
        "        # c is the last cell state\n",
        "        packed_outputs, (h, _) = self.lstm(pack_embedded)\n",
        "        \n",
        "        # h: [num_layers * num_directions, batch_size, hidden dim]\n",
        "        \n",
        "        # it happens that because packed_outputs is all hidden states....some hidden states near the end is\n",
        "        # hidden state for padding, pytorch guys help you\n",
        "        # by using this pad_packed_sequence, then all the hidden states will only be not padding....\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first = True)\n",
        "        # output: [batch size, seq len, direction * hidden sim]\n",
        "        \n",
        "        # last hidden state - concat last forward and backward states\n",
        "        last_hidden_state = torch.cat((h[-1, :, :], h[-2, :, :]), dim = 1)\n",
        "        # last_hidden_state: [batch_size, hidden_dim * 2]\n",
        "        \n",
        "        # for sentiment analysis.....what should i sent to my linear layer...\n",
        "        return self.fc(last_hidden_state)  # [batch_size, output_dim]==> [batch_size, 4]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim  = 13767 # len(vocab)\n",
        "hid_dim    = 256\n",
        "emb_dim    = 300\n",
        "output_dim = 2\n",
        "\n",
        "#for biLSTM\n",
        "num_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.2\n",
        "\n",
        "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)"
      ],
      "metadata": {
        "id": "mRpgDSbyJFNe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Model"
      ],
      "metadata": {
        "id": "-Qbz9x3jJ5R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = f'{model.__class__.__name__}.pt'\n",
        "# model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "r8enIEWbJ92q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if next(model.parameters()).is_cuda:\n",
        "    model = model.cpu()"
      ],
      "metadata": {
        "id": "rZkSi9bPKBdB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking Posts and Sentiments "
      ],
      "metadata": {
        "id": "MXwBYq08SbHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, text_length):\n",
        "    with torch.no_grad():\n",
        "        output = model(text, text_length).squeeze(1)\n",
        "        predicted = torch.max(output.data, 1)[1]\n",
        "        return predicted"
      ],
      "metadata": {
        "id": "EY4SLk8uSlbt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = []\n",
        "\n",
        "for title in titles:\n",
        "    text = torch.tensor(text_pipeline(title))\n",
        "    text = text.reshape(1, -1)\n",
        "    text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
        "    senti_pred = predict(text, text_length)\n",
        "    sentiment.append(int(senti_pred[0]))"
      ],
      "metadata": {
        "id": "3ulyWfDgSpBH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pd.DataFrame()\n",
        "\n",
        "pred['Title'] = titles\n",
        "pred['Sentiment'] = sentiment\n",
        "\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dP-yPPpSo80",
        "outputId": "697a66a7-d359-4a4e-f0e5-c124ef2657c4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Title  Sentiment\n",
            "0   Got accepted into medical school and my parent...          0\n",
            "1          Goodbye surface ‚Äúpro‚Äù 7, hello bankruptcy.          0\n",
            "2   Surprised my girlfriend with a MacBook Air for...          0\n",
            "3   Unpopular Opinion / Rant: This subreddit has b...          0\n",
            "4                           Loving my new retro skins          1\n",
            "5                                    So many macbooks          0\n",
            "6   Upgraded! From 2006 Intel MacBook Pro to 2020 ...          0\n",
            "7                       MacBook pro 16 m1pro unboxed.          0\n",
            "8                                           The M1 rn          0\n",
            "9   ü§¶‚Äç‚ôÇÔ∏è All these M1 posts are having me regret b...          0\n",
            "10                                      All the time.          1\n",
            "11         Mac with extend Monitor - Nice combination          0\n",
            "12  Wasn‚Äôt a fan of apple products but everything ...          0\n",
            "13                                We are not the same          0\n",
            "14                                              me rn          0\n",
            "15                               The new MacBook Air.          1\n",
            "16  Can we just talk about how hard the product in...          0\n",
            "17          How deep are you in the apple ecosystem ?          0\n",
            "18                                 Finally it‚Äôs here!          0\n",
            "19  my parents bought me the m1 macbook air as a c...          0\n",
            "20  Me, an M1 MacBook Air user trying to convince ...          0\n",
            "21         Who else misses the Glowing Apple Logo? :(          0\n",
            "22            So after much debate between Pro & Air‚Ä¶          0\n",
            "23  Thank you 2010 MacBook Pro for your decade of ...          0\n",
            "24         Mac OS, stop it. Please. Stop the torture.          0\n",
            "25                      Something good arrived today!          1\n",
            "26  This is my first Mac! I always wanted one sinc...          0\n",
            "27  I was told you might like this. My white MacBook.          1\n",
            "28            first ever mac i've owned, silver m1 :)          0\n",
            "29         I actually feel sorry for some CURTAIN ppl          0\n",
            "30  What an absolute beast! MBA M1, 8/8 Core, 16GB...          0\n",
            "31          Her first MacBook! #BirthdayGift #MBA #M1          0\n",
            "32  M1 MacBook Air, emulating ARM Windows 10, emul...          0\n",
            "33   Upgraded from my 18‚Äô Air to the new Midnight M2.          0\n",
            "34                        So beautiful MacBook Air m2          0\n",
            "35  Greetings! Just got my new MacBook Air M1! I'v...          0\n",
            "36                        Everybody in this subreddit          0\n",
            "37                                    Apple vs Huawei          0\n",
            "38                                     Slight upgrade          0\n",
            "39                          My first MacBook is here!          0\n",
            "40  Public school bought M1 MacBook Airs for everyone          0\n",
            "41  My MacBook 2015 just blew up in front of my fa...          0\n",
            "42  This is bonkers - ordered at 1:45pm, courier d...          0\n",
            "43  I haven‚Äôt charged my M1 in a week, and it stil...          0\n",
            "44                      Somebody forgot their MacBook          0\n",
            "45                                    My first mac :)          0\n",
            "46  New machine delivered at 7:56am UK time it fee...          0\n",
            "47   I didn‚Äôt know my iPad and laptop could do this ü§Ø          0\n",
            "48                           Just got it in the mail!          0\n",
            "49                         MacBook‚Ä¶ without the Book?          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the Count of Positive and Negative Posts"
      ],
      "metadata": {
        "id": "qwjnqDlrS63O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentiment_count = {\"positive\": 0, \"negative\": 0}\n",
        "for senti in sentiment:\n",
        "    if senti == 1:\n",
        "        sentiment_count[\"positive\"] += 1\n",
        "    else:\n",
        "        sentiment_count[\"negative\"] += 1\n",
        "\n",
        "labels = list(sentiment_count.keys())\n",
        "count  = list(sentiment_count.values())\n",
        "\n",
        "plt.figure(figsize = (8, 8))\n",
        "plt.bar(labels, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "mlehRFB5So4V",
        "outputId": "b896da41-bdd1-48e4-dd31-56756fca31b2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 2 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top Words inside Positive and Negative Comments"
      ],
      "metadata": {
        "id": "rhLHnWFtTt-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_word_count = {}\n",
        "neg_word_count = {}\n",
        "\n",
        "def incr_word_count(text, word_count):\n",
        "    if text in word_count:\n",
        "        word_count[text] += 1\n",
        "    else:\n",
        "        word_count[text] = 1\n",
        "\n",
        "for idx in range(len(pred)):\n",
        "    text_list  = tokenizer(pred[\"Title\"][idx])\n",
        "    senti_pred = pred[\"Sentiment\"][idx]\n",
        "    \n",
        "    for text in text_list:\n",
        "        text = text.lower()\n",
        "        \n",
        "        if senti_pred == 1:\n",
        "            incr_word_count(text, pos_word_count)\n",
        "        elif senti_pred == 0:\n",
        "            incr_word_count(text, neg_word_count)"
      ],
      "metadata": {
        "id": "6GPU6hlMS2fl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positive words\n",
        "sorted_pos_word_count = dict(sorted(pos_word_count.items(), key=lambda item: item[1]))"
      ],
      "metadata": {
        "id": "f3gPalUES2W8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top words inside positive posts:\")\n",
        "print(list(sorted_pos_word_count.keys())[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za2nSp8lUFZ7",
        "outputId": "23019874-2bb8-41aa-ffc5-01ce79c402c2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words inside positive posts:\n",
            "['you', 'might', 'like', 'this', 'white', 'my', 'new', 'the', 'macbook', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# negative words\n",
        "sorted_neg_word_count = dict(sorted(neg_word_count.items(), key=lambda item: item[1]))"
      ],
      "metadata": {
        "id": "aLdt0_D8URFU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top words inside negative posts:\")\n",
        "print(list(sorted_neg_word_count.keys())[-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4RdZ5VIUaj-",
        "outputId": "15baf1c7-91c4-4a14-8947-de542e7a9a8c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words inside negative posts:\n",
            "['a', 'air', 'i', '!', 'my', 'm1', ',', 'the', '.', 'macbook']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Top words inside negative posts:\")\n",
        "print(list(sorted_neg_word_count.keys())[-20:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m58rZqKUhUL",
        "outputId": "9483062b-9a38-40a2-e862-d1e2e8ef19bb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words inside negative posts:\n",
            "['?', 'first', 'got', 'and', 'for', 'me', 'this', 'pro', 'it', 'in', 'a', 'air', 'i', '!', 'my', 'm1', ',', 'the', '.', 'macbook']\n"
          ]
        }
      ]
    }
  ]
}
